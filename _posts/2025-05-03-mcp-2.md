---
layout: post
title: "用 FastAPI 构建大模型中控协议（MCP）系统：从意图识别到工具调用"
date: 2025-05-03 10:00:00 +0800
categories: [AI]
tags: [Claude, OpenAi, 大模型, mcp]
img:  posts/20250503/mcp.jpg
---


## 📚 文章结构概览

1. 引言：MCP 协议的背景与目标  
2. 系统架构总览与模块划分  
3. Server 实现详解  
4. Client 请求逻辑分析  
5. 意图提取模块（LLM）与工具调用的集成  
6. 实际运行演示  
7. 后续进阶计划与总结

# 1️⃣ 引言：MCP 协议的背景与目标

在大模型与工具集成的实践中，如何高效地从自然语言输入中提取意图，并调用对应的功能模块，是构建具备真实生产力的大模型系统的关键。为此，我们设计了一个轻量、可扩展的中控协议（MCP，Model Control Protocol），以标准化用户输入、意图识别、工具调用与响应生成的全过程。

本设计的应用主要是一个demo，作为对mcp开发的支持，避免纸上谈兵的理论输出，通过编码实践来深入了解mcp，明确mcp可以完成的内容和功能。确保在后续的mcp的应用的时候可以争取认识mcp，做出正确的选型。

本项目采用 Python + FastAPI 实现，具备以下核心能力：
	•	✅ 支持自然语言输入到意图的转换
	•	✅ 通过协议标准封装上下文、请求与响应
	•	✅ 可扩展的工具调用体系
	•	✅ 具备清晰的 client/server 架构，方便前后分离

# 2️⃣ 项目架构总览与模块划分

项目结构如下图所示：

```text
WEATHER-SCP-SDK/
├── llm/                     # 模型层：意图抽取
│   └── model_wrapper.py
├── tools/                   # 工具层：天气查询等 mock 实现
│   └── weather_tool.py
├── protocol.py              # 协议定义：MCPRequest, MCPReply 等
├── server.py                # 服务端接口（FastAPI）
├── client.py                # 客户端测试脚本
├── requirements.txt         # 依赖
└── readme.md
```

>> 📌 说明：我们以“天气查询”作为工具调用的示例，后续可拓展为更多工具类型（如节假日查询、航班信息等）。

# 3️⃣ Server 实现详解：请求到响应的全过程

服务端采用 FastAPI 构建，监听 /mcp 接口，接收结构化的 MCPRequest 请求。其完整逻辑如下：

```python
from fastapi import FastAPI
from protocol import MCPRequest, MCPReply, MCPIntent, MCPResponse, MCPAction
from llm.model_wrapper import extract_weather_intent
from tools.weather_tool import mock_query_weather

app = FastAPI()

@app.post("/mcp")
def handle_mcp(request: MCPRequest) -> MCPReply:
    user_input = request.context.history[-1]
    intent_dict = extract_weather_intent(user_input)
    intent = MCPIntent(**intent_dict)

    location = intent.entities.get("location")
    date = intent.entities.get("date")
    weather_text = mock_query_weather(location, date)

    response = MCPResponse(type="text", content=weather_text)
    action = MCPAction(
        type="tool_call",
        tool="weather_api",
        parameters={"location": location, "date": date}
    )

    return MCPReply(response=response, actions=[action])

```

> ✅ 服务端职责：
>•	拆解用户输入 → 提取意图
>•	调用对应工具 → 返回结构化响应


# 4️⃣ Client 请求逻辑分析

客户端用于构造 MCPRequest 请求，并通过 HTTP POST 调用服务端接口：

```python
import requests, uuid
from protocol import MCPRequest, MCPUser, MCPContext

def send_mcp_request(user_input: str):
    request = MCPRequest(
        session_id=str(uuid.uuid4()),
        user=MCPUser(id="user-001", intent=None),
        context=MCPContext(history=[user_input])
    )

    response = requests.post("http://127.0.0.1:8000/mcp", json=request.model_dump())
    print("🤖:", response.json()["response"]["content"])

if __name__ == "__main__":
    send_mcp_request("帮我查一下明天北京的天气")
```

>💡 客户端核心思路是：让服务端完成意图识别与工具调用，客户端仅负责发送自然语言请求和展示结果。


# 5️⃣ LLM 模块与工具调用机制

在这个系统中，大模型相关的处理被封装在 llm/model_wrapper.py 中，用于完成意图的抽取。这里我们以天气查询为例，将自然语言输入解析为一个结构化的 MCPIntent 对象。

📌 extract_weather_intent 实现示例：


```python
# llm/model_wrapper.py

def extract_weather_intent(user_input: str) -> dict:
    # 模拟 LLM 对输入的解析，这里直接 hardcode 便于调试
    if "北京" in user_input and "明天" in user_input:
        return {
            "name": "query_weather",
            "entities": {
                "location": "北京",
                "date": "明天"
            }
        }
    return {
        "name": "unknown",
        "entities": {}
    }
```

> 🚧 实际部署时，这里可以替换为调用 LLM（如 OpenAI、ChatGLM、qwen）接口，并使用 prompt 或 schema 做意图抽取。

🔧 工具模块：模拟天气 API

为了模拟真实场景，我们引入了 mock_query_weather，位于 tools/weather_tool.py，用于返回模拟数据：

```python
# tools/weather_tool.py

def mock_query_weather(location: str, date: str) -> str:
    return f"{date} {location} 的天气是晴，气温 25°C。"
```

> 这种方式便于后期切换成真实接口（如和风天气、百度天气、国家气象局等）。在实际使用的时候可以添加必要的天气接口、在线查询等方式进行替换

# 6️⃣ 实际运行效果
运行命令如下：

```shell
# 启动服务端
$ uvicorn server:app --reload

# 运行客户端
$ python client.py
```

控制台输出如下：
```text
🤖: 正在发送请求...
🤖: 请求已发送，等待响应...
🟡 状态码: 200
🟡 返回体: {"response": {"type": "text", "content": "明天 北京 的天气是晴，气温 25°C。"}, "actions": [{"type": "tool_call", "tool": "weather_api", "parameters": {"location": "北京", "date": "明天"}}]}
🤖: 明天 北京 的天气是晴，气温 25°C。
```


# 7️⃣ 后续进阶计划与总结

在本项目中，我们构建了一个简洁的 MCP（Model Control Protocol）原型，实现了从自然语言指令到意图识别、工具调用再到响应生成的完整链路闭环。这一流程看似简单，实际上承载着构建“可落地的大模型系统能力”的关键机制。

从表层看，MCP 与 OpenAI 的 Function Call、Claude 的 tool use，乃至于直接写 prompt 来控制工具行为，似乎实现目标一致，都是实现“让大模型调用外部能力”。但从工程视角深入观察，MCP 的设计提供了三点关键价值：

✅ 1. 标准化语言交互协议

传统的 prompt 编写或 Function Schema 使用通常是**“任务型一对一封装”** —— 为一个具体场景设计一个 prompt 或 schema。但这在多业务线、多开发团队协同的大模型平台建设中显得杂乱、难以维护。

MCP 则通过统一的 MCPRequest / MCPReply / MCPIntent / MCPAction 等协议抽象，建立了统一的交互语义模型。这意味着，不同工具、不同业务逻辑都可以用相同的交互模式来驱动，极大地降低了系统的复杂度与耦合度。

✅ 2. 可复制的组件化结构

MCP 将复杂的大模型调用流程切分为标准模块（如 LLM 层、工具调用层、协议解析层等），并通过组件化组合形成完整业务流。这种设计意味着：
	•	新业务只需新增工具模块和意图解析逻辑，无需重写整体结构
	•	可以在不同项目中快速复用已有模块，极大提升工程效率
	•	MCP 系统自身可以被“平台化”，为多个业务提供统一中控服务

🧩 本项目中的 “天气工具” 只是一个起点，未来可拓展为“多工具、多意图、多响应模态”的工具管理平台。


✅ 3. 衔接 LLM 与工具生态的中间件层

MCP 并不试图替代提示词，也不局限于某个 LLM 接口，它更像是LLM 和现实世界能力之间的桥梁。你可以将 LLM 理解为“通用语言处理引擎”，而 MCP 是构建在其之上的业务控制协议层。

在这个意义上，MCP 提供了以下能力：
	•	🌉 解耦业务功能与模型细节：Prompt 的更新或模型替换不会影响工具调用逻辑
	•	⚙️ 支持多模型切换与调度：未来甚至可以对接多个 LLM，根据场景切换最优模型
	•	🔒 增强可控性与安全性：通过中间协议过滤非结构化输入，规避模型直接执行的风险


🚀 后续演进方向

为了进一步提升 MCP 的实际价值，我们计划从以下几个方向推进：
	•	🔍 引入真实意图识别 LLM，如 Claude、Qwen、ChatGLM，并探索 Schema-Guided Prompting
	•	🧠 构建一个可注册的 Tool Registry 模块，支持动态加载工具
	•	🔁 支持多轮对话意图管理，实现上下文融合与任务续连
	•	🌐 封装为 SDK 或 API 服务，供业务系统直接集成
	•	📊 加入日志追踪与指标统计，形成对工具调用效果的可视化反馈


✅ 总结一句话：

MCP 不是替代提示词或 Function Call 的技术，而是从工程标准化角度，对“模型+工具”这一融合形态的强力支撑。
它帮助我们从“用大模型完成一个任务”进化到“系统性构建多任务中控平台”的方向。